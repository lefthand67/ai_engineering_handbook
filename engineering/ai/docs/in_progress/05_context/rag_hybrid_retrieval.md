# Гибридный поиск в RAG

> Владелец: Вадим Рудаков, lefthand67@gmail.com
> Версия: 0.1.0

Гибридный поиск (hybrid retrieval) — это метод, который объединяет семантический и ключевой поиск, повышая качество Retrieval-Augmented Generation (RAG) систем. Такой подход одновременно обеспечивает гибкость понимания смысла и точность совпадений, что особенно важно в технических, юридических и многодоменных задачах.  

## Что такое гибридный поиск
- Semantic search — семантический поиск, использует эмбеддинги для понимания смысла и контекста. Позволяет находить документы с похожим значением, даже если формулировки запроса отличаются.  
- Keyword search — ключевой поиск, основан на точном совпадении терминов, акронимов, кодов и идентификаторов (например, с помощью BM25). Он гарантирует нахождение строгих совпадений, которые семантические модели часто упускают.  

Комбинация этих методов повышает полноту и точность поиска, компенсируя слабые стороны друг друга.

## Зачем это нужно
- Семантика хорошо справляется с перефразированием и синонимами, но может пропустить редкие термины.  
- Ключевой поиск обеспечивает точные совпадения, но «слепнет» при изменении формулировок.  
- Гибрид закрывает обе эти проблемы, делая систему более устойчивой при работе с разрозненной информацией.

## Как работает гибридный retrieval
1. Запрос одновременно отправляется в векторный индекс и keyword-индекс (например, BM25).  
2. Оба механизма возвращают списки кандидатов с оценками релевантности.  
3. Оценки объединяются (например, взвешенным средним с весами или моделью ранжирования learning-to-rank).  
4. Итоговый список документов передаётся в LLM для генерации ответа.  

## Преимущества
- Более высокая полнота (**recall**) и точность (**precision**).  
- Устойчивость к вариативности языка и редким терминам.  
- Эффективность при работе с логами, техническими кодами и текстами одновременно.  
- Снижение риска «слепых зон» за счёт сочетания подходов.  

## Пример кода (Python)

```python
from rank_bm25 import BM25Okapi
from sentence_transformers import SentenceTransformer
import numpy as np

# Corpus
docs = ["Error code 1234", "Timeout occurred", "User login failed"]

# BM25 index
bm25 = BM25Okapi([doc.split() for doc in docs])

# Embeddings
model = SentenceTransformer('all-MiniLM-L6-v2')
doc_embeddings = model.encode(docs)

# Query
query = "login error"
bm25_scores = bm25.get_scores(query.split())

query_emb = model.encode([query])[0]
cosine_sims = np.dot(doc_embeddings, query_emb) / (
    np.linalg.norm(doc_embeddings, axis=1) * np.linalg.norm(query_emb)
)

# Weighted fusion
weight_sem, weight_key = 0.7, 0.3
combined = weight_sem * cosine_sims + weight_key * bm25_scores

# Ranking
top_indices = combined.argsort()[::-1]
top_docs = [docs[i] for i in top_indices]
print(top_docs)
```

## Рекомендации и подводные камни
- **Настройка весов**: фиксированные веса могут плохо обобщаться, лучше подбирать или обучать модель ранжирования.  
- **Нормализация**: BM25 и косинусные сходства находятся в разных диапазонах, их нужно приводить к единой шкале.  
- **Производительность**: два поисковых механизма удваивают нагрузку, рекомендуется предвычисление индексов и эмбеддингов.  
- **Реранкинг**: для высокоточных задач используйте кросс-энкодеры для переоценки списка кандидатов.  
- **Метрики**: проверяйте и полноту, и точность — семантик может казаться «умнее», но упускать нужные документы.  

Ментальная модель:  
Семантический поиск — это *друг, который понимает идею запроса, даже если слова другие*.  
Ключевой поиск — это *детектив, который точно находит нужные символы или термины*.  
Вместе они делают систему гораздо надёжнее.  

***

PostgreSQL с расширением **pgvector** и организация гибридного retrieval внутри RAG-пайплайна.

## Подготовка окружения
1. Установите расширение **pgvector**:  
```sql
CREATE EXTENSION IF NOT EXISTS vector;
```

Оно добавляет новый тип данных `vector` и функции для поиска по косинусной или евклидовой метрике.

2. Для keyword-поиска важно, чтобы была настроена поддержка полнотекстового поиска в PostgreSQL:  
   - `GIN` индексы для полнотекстового поиска (`tsvector`)  
   - настроенные словари при необходимости (русский, английский или смешанный).

## Структура таблицы документов
```sql
CREATE TABLE documents (
    id SERIAL PRIMARY KEY,
    content TEXT,
    embedding VECTOR(384),   -- например, размер эмбеддинга от all-MiniLM-L6-v2
    tokens TSVECTOR
);
```

- Поле `embedding` хранит семантический вектор.  
- Поле `tokens` используется для индексного поиска по ключевым словам.  


## Индексы

```sql
-- Индекс для векторных поисков
CREATE INDEX ON documents USING ivfflat (embedding vector_cosine_ops)
WITH (lists = 100);

-- Индекс для полнотекстового поиска
CREATE INDEX ON documents USING GIN (tokens);
```

- `ivfflat` обеспечивает быстрый поиск по векторам.  
- `GIN` делает быстрый keyword-поиск по tsvector.  

## Добавление данных
```sql
-- Пример вставки
INSERT INTO documents (content, embedding, tokens)
VALUES (
    'User login failed',
    '[0.0123, 0.9876, ...]',                -- сюда отправляется готовый вектор
    to_tsvector('english', 'User login failed')
);
```

Вектор обычно вычисляется заранее (Python/LLM пайплайн), и в БД записывается готовым массивом.

## Запрос гибридного поиска

### Шаг 1. Семантический поиск

```sql
SELECT id, content,
       1 - (embedding <=> '[вектор_запроса]') AS sem_score
FROM documents
ORDER BY sem_score DESC
LIMIT 20;
```

Здесь `<=>` — оператор косинусного расстояния (pgvector автоматически поддерживает его).  

### Шаг 2. Keyword-поиск

```sql
SELECT id, content,
       ts_rank(tokens, to_tsquery('english', 'login & error')) AS key_score
FROM documents
ORDER BY key_score DESC
LIMIT 20;
```

### Шаг 3. Объединение результатов

Postgres сам по себе не умеет "склеивать" векторные + полнотекстовые запросы в едином операторе. Обычно делают так:
- Выполняют оба запроса.  
- Склеивают результаты во временную таблицу или CTE с нормализацией.  

Пример (нормализация через `min-max`):

```sql
WITH sem AS (
    SELECT id, (1 - (embedding <=> '[вектор_запроса]')) AS sem_score
    FROM documents
    LIMIT 50
),
keyw AS (
    SELECT id, ts_rank(tokens, to_tsquery('english', 'login & error')) AS key_score
    FROM documents
    LIMIT 50
),
combined AS (
    SELECT d.id, d.content,
        COALESCE(s.sem_score, 0) AS sem_score,
        COALESCE(k.key_score, 0) AS key_score,
        (0.7 * COALESCE(s.sem_score, 0) + 0.3 * COALESCE(k.key_score, 0)) AS hybrid_score
    FROM documents d
    LEFT JOIN sem s ON d.id = s.id
    LEFT JOIN keyw k ON d.id = k.id
)
SELECT * FROM combined
ORDER BY hybrid_score DESC
LIMIT 10;
```

## Best Practices
- **Нормализация обязательна**: значения cosine similarity (0–1) и `ts_rank` (обычно более «шумные») нужно приводить к единой шкале.  
- **Тюнинг списков** (`lists`) в ivfflat сильно влияет на recall/latency. Здесь важно тестировать.  
- **to_tsquery vs plainto_tsquery**: первый даёт более точные логические запросы, второй автоматически разбивает текст. Для гибридного варик лучший компромисс — `plainto_tsquery`.  
- **Кеширование эмбеддингов**: вычисляйте модели (например, SentenceTransformers, OpenAI Embeddings) вне БД и загружайте пакетно.  

## Архитектура пайплайна RAG с PostgreSQL
1. **Индексирование данных**: загрузка текста, построение эмбеддингов, заполнение `tsvector`.  
2. **Запрос**:
   - LLM формулирует поисковый запрос (иногда в двух вариантах: «для семантики» и «для keyword»).  
   - В коде Python → выполняется семантический и ключевой поиск через SQL.  
   - Результаты агрегируются и нормализуются.  
3. **Реранкинг (опционально)**: переоценка кандидатов кросс-энкодером.  
4. **Ответ LLM** на основе лучших документов.
